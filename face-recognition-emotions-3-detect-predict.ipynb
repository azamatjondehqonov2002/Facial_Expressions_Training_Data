{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Face detection","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"# face detection function using MTCNN pre-trained model\n\n!pip install mtcnn\n#!apt-get update\n#!apt-get install -y libgl1-mesa-glx\n\nimport cv2\nfrom mtcnn import MTCNN\nfrom IPython.display import Image\n\ndef detect_faces(image_path, detection_confidence=0.99, min_face_size=10):\n    # Load the image using OpenCV\n    image = cv2.imread(image_path)\n\n    # Create an MTCNN detector instance\n    detector = MTCNN()\n\n    # Use the detector to detect faces in the image\n    faces = detector.detect_faces(image)\n\n    # Draw a square around each face\n    for face in faces:\n        if face['confidence'] < detection_confidence:\n            continue\n        x, y, width, height = face['box']\n        if min(width, height) < min_face_size:\n            continue\n        face_size = max(width, height)\n        x, y = x + (width - face_size) // 2, y + (height - face_size) // 2\n        cv2.rectangle(image, (x, y), (x + face_size, y + face_size), (127, 255, 0), 1)\n\n    # Save the image with the detected faces to a file\n    cv2.imwrite(\"detected_faces.jpg\", image)\n\n    # Return the path to the saved file\n    return \"detected_faces.jpg\"\n\n","metadata":{"execution":{"iopub.status.busy":"2023-02-20T14:00:16.620822Z","iopub.execute_input":"2023-02-20T14:00:16.621667Z","iopub.status.idle":"2023-02-20T14:00:32.691978Z","shell.execute_reply.started":"2023-02-20T14:00:16.621592Z","shell.execute_reply":"2023-02-20T14:00:32.690783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Image\n\n# Detect faces in the image of various emotions\nimage_path = detect_faces('/kaggle/input/testimages/35_emotions.jpeg')\n\n# Display the saved image\nImage(filename=image_path)","metadata":{"execution":{"iopub.status.busy":"2023-02-20T14:00:32.693872Z","iopub.execute_input":"2023-02-20T14:00:32.694150Z","iopub.status.idle":"2023-02-20T14:00:34.846673Z","shell.execute_reply.started":"2023-02-20T14:00:32.694120Z","shell.execute_reply":"2023-02-20T14:00:34.844726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Look, MTCNN model is unable to identify some faces","metadata":{}},{"cell_type":"markdown","source":"# Model for Emotions prediction","metadata":{}},{"cell_type":"code","source":"# let's see what is in our AffectNet dataset\n\nimport pandas as pd\nimport matplotlib.pyplot as plt  # plot\nimport os\nfrom os.path import join\n\npath = ('/kaggle/input/affectnet-training-data/')\nfile = (path + 'labels.csv')\ndf = pd.read_csv(file)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-02-20T14:00:34.847899Z","iopub.execute_input":"2023-02-20T14:00:34.848077Z","iopub.status.idle":"2023-02-20T14:00:34.924942Z","shell.execute_reply.started":"2023-02-20T14:00:34.848055Z","shell.execute_reply":"2023-02-20T14:00:34.924298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display random images\n\nimport random\n\nfig, axs = plt.subplots(2, 4, sharey=True, constrained_layout=True, num=None, \n                        figsize=(10, 5), dpi=80, facecolor='gray', edgecolor='k')\nfig.suptitle(\"Sample Faces and Labels\")\naxs = axs.flatten()\n\n\nfor i in range(8):\n    idx = random.randint(0, len(df)-1)  # randomly select an index\n    img_path = path + df['pth'][idx]\n    img = cv2.imread(img_path)  # read image\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # convert to BGR to RGB\n    axs[i].imshow(img)\n    axs[i].set_title(df['label'][idx])\n    axs[i].axis('off')","metadata":{"execution":{"iopub.status.busy":"2023-02-20T14:00:34.926557Z","iopub.execute_input":"2023-02-20T14:00:34.926834Z","iopub.status.idle":"2023-02-20T14:00:35.742405Z","shell.execute_reply.started":"2023-02-20T14:00:34.926812Z","shell.execute_reply":"2023-02-20T14:00:35.741946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"AffectNet is a large database of faces labeled by \"affects\" (psychological term for facial expressions). In order to accommodate common memory constraints, the resolution was reduced down to 96x96. Meaning that all images are exactly 96x96 pixels.","metadata":{}},{"cell_type":"markdown","source":"# Load images and label categories","metadata":{}},{"cell_type":"code","source":"# 1. define functions to pre-process and load images into arrays / label from folders\n\nimport cv2\nimport numpy as np\nfrom tensorflow.keras.utils import to_categorical\nimport os\n\nINPUT_PATH = \"/kaggle/input/affectnet-training-data/\"\nEMOTIONS = [f.name for f in os.scandir(INPUT_PATH) if f.is_dir()]\nIMAGE_SIZE = (96, 96)\n\nprint(EMOTIONS)\n\ndef image_generator(input_path, emotions, image_size):\n    for index, emotion in enumerate(emotions):\n        for filename in os.listdir(os.path.join(input_path, emotion)):\n            img = cv2.imread(os.path.join(input_path, emotion, filename))\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n            #img = cv2.resize(img, image_size)\n            #img = img.astype('float32') / 255.0  # Normilize\n            yield img, index\n\ndef load_images(input_path, emotions, image_size):\n    X, y = [], []\n    for img, label in image_generator(input_path, emotions, image_size):\n        X.append(img)\n        y.append(label)\n    X = np.array(X)\n    y = to_categorical(np.array(y))\n    return X, y","metadata":{"execution":{"iopub.status.busy":"2023-02-20T14:00:35.743256Z","iopub.execute_input":"2023-02-20T14:00:35.743479Z","iopub.status.idle":"2023-02-20T14:00:35.754061Z","shell.execute_reply.started":"2023-02-20T14:00:35.743459Z","shell.execute_reply":"2023-02-20T14:00:35.753298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the images \nX, y = load_images(INPUT_PATH, EMOTIONS, IMAGE_SIZE)\ninput_shape = X[0].shape\n#input_shape = (96,96,1) ","metadata":{"execution":{"iopub.status.busy":"2023-02-20T14:00:35.755037Z","iopub.execute_input":"2023-02-20T14:00:35.755238Z","iopub.status.idle":"2023-02-20T14:05:11.525448Z","shell.execute_reply.started":"2023-02-20T14:00:35.755194Z","shell.execute_reply":"2023-02-20T14:05:11.524705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# choose a random image index\nidx = np.random.randint(len(X))\n\n# display the image and its corresponding label from arrays\nplt.imshow(X[idx])\nplt.title(EMOTIONS[np.argmax(y[idx])])\nplt.axis('off')  # remove the grid\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-20T14:05:11.526393Z","iopub.execute_input":"2023-02-20T14:05:11.526619Z","iopub.status.idle":"2023-02-20T14:05:11.626753Z","shell.execute_reply.started":"2023-02-20T14:05:11.526598Z","shell.execute_reply":"2023-02-20T14:05:11.626274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train test split pre-processed data\n\n!pip install scikit-learn\n\nfrom sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)","metadata":{"execution":{"iopub.status.busy":"2023-02-20T14:05:11.627769Z","iopub.execute_input":"2023-02-20T14:05:11.628061Z","iopub.status.idle":"2023-02-20T14:05:18.409771Z","shell.execute_reply.started":"2023-02-20T14:05:11.628025Z","shell.execute_reply":"2023-02-20T14:05:18.408764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Emotions prediction model","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nprint(\"Num TPUs Available: \", len(tf.config.list_logical_devices('TPU')))\n","metadata":{"execution":{"iopub.status.busy":"2023-02-20T14:05:18.411066Z","iopub.execute_input":"2023-02-20T14:05:18.411261Z","iopub.status.idle":"2023-02-20T14:05:18.416412Z","shell.execute_reply.started":"2023-02-20T14:05:18.411228Z","shell.execute_reply":"2023-02-20T14:05:18.415153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# detect and init the TPU\n\nimport tensorflow as tf\n\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","metadata":{"execution":{"iopub.status.busy":"2023-02-20T14:05:18.420930Z","iopub.execute_input":"2023-02-20T14:05:18.421157Z","iopub.status.idle":"2023-02-20T14:05:24.146050Z","shell.execute_reply.started":"2023-02-20T14:05:18.421136Z","shell.execute_reply":"2023-02-20T14:05:24.145274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# our model_4 taken from Experiments notebook\n\n!pip install keras\n\n# instantiating the model in the strategy scope creates the model on the TPU\nwith tpu_strategy.scope():\n\n    from tensorflow.keras.models import Sequential\n    from tensorflow.keras.layers import Conv2D, Dropout, BatchNormalization, Flatten, Dense, MaxPool2D\n    from tensorflow.keras.regularizers import l2\n    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n    from tensorflow.keras.optimizers import Adam\n\n    model_4 = Sequential()\n\n    model_4.add(Conv2D(32, (3,3), activation=\"selu\", input_shape=input_shape))\n    model_4.add(BatchNormalization())\n    model_4.add(MaxPool2D(pool_size=(2,2)))\n    model_4.add(Dropout(0.3))\n\n    model_4.add(Conv2D(64, (3,3), activation=\"selu\"))\n    model_4.add(BatchNormalization())\n    model_4.add(Conv2D(64, (3,3), activation=\"selu\"))\n    model_4.add(BatchNormalization())\n    model_4.add(MaxPool2D(pool_size=(2,2)))\n    model_4.add(Dropout(0.4))\n\n    model_4.add(Conv2D(128, (3,3), activation=\"selu\"))\n    model_4.add(BatchNormalization())\n    model_4.add(Conv2D(128, (3,3), activation=\"selu\"))\n    model_4.add(BatchNormalization())\n    model_4.add(MaxPool2D(pool_size=(2,2)))\n    model_4.add(Dropout(0.5))\n\n    model_4.add(Conv2D(256, (3,3), activation=\"selu\"))\n    model_4.add(BatchNormalization())\n    model_4.add(Conv2D(256, (3,3), activation=\"selu\"))\n    model_4.add(BatchNormalization())\n    model_4.add(MaxPool2D(pool_size=(2,2)))\n    model_4.add(Dropout(0.6))\n\n    model_4.add(Flatten())\n    model_4.add(Dense(128, activation='selu', kernel_regularizer=l2(0.01)))\n    model_4.add(BatchNormalization())\n    model_4.add(Dropout(0.5))\n    model_4.add(Dense(8, activation='softmax'))\n\n    model_4.compile(optimizer=Adam(learning_rate=0.001), \n                    loss='categorical_crossentropy', \n                    metrics=['accuracy'],\n                    steps_per_execution=32)\n\n    model_4.summary()","metadata":{"execution":{"iopub.status.busy":"2023-02-20T14:05:24.147544Z","iopub.execute_input":"2023-02-20T14:05:24.147778Z","iopub.status.idle":"2023-02-20T14:05:31.498350Z","shell.execute_reply.started":"2023-02-20T14:05:24.147749Z","shell.execute_reply":"2023-02-20T14:05:31.497093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n\nBATCH_SIZE = 16 * tpu_strategy.num_replicas_in_sync\n\nhistory = model_4.fit(X_train, y_train, batch_size=BATCH_SIZE,\n                    epochs=35,\n                    validation_data=(X_test, y_test),\n                    \n                    callbacks = [EarlyStopping(patience=10, monitor='val_loss', mode='min'), \n                                 ReduceLROnPlateau(monitor='val_loss', \n                                                   factor=0.5, \n                                                   patience=2, \n                                                   verbose=1),\n                                 ModelCheckpoint('best_model.h5', \n                                                 save_best_only=True, \n                                                 save_weights_only=True, \n                                                 monitor='val_accuracy', \n                                                 mode='max')],\n                    verbose=1)","metadata":{"execution":{"iopub.status.busy":"2023-02-20T14:05:31.499707Z","iopub.execute_input":"2023-02-20T14:05:31.499921Z","iopub.status.idle":"2023-02-20T14:06:59.906894Z","shell.execute_reply.started":"2023-02-20T14:05:31.499898Z","shell.execute_reply":"2023-02-20T14:06:59.905920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(history.history).plot();","metadata":{"execution":{"iopub.status.busy":"2023-02-20T14:06:59.908159Z","iopub.execute_input":"2023-02-20T14:06:59.908382Z","iopub.status.idle":"2023-02-20T14:07:00.150124Z","shell.execute_reply.started":"2023-02-20T14:06:59.908352Z","shell.execute_reply":"2023-02-20T14:07:00.149080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculates the false positive rate, true positive rate, and AUC score\n\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\nimport matplotlib.pyplot as plt\n\n# Make predictions\ny_pred = model_4.predict(X_test)\n\n# Compute ROC curve and ROC AUC for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(8):\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n# Compute micro-average ROC curve and ROC AUC score\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_pred.ravel())\nroc_auc[\"micro\"] = roc_auc_score(y_test, y_pred, multi_class='ovr')\n\n# Plot the ROC curves for each class and the micro-average ROC curve\nplt.figure(figsize=(8, 6))\nlw = 2\nplt.plot(fpr[\"micro\"], tpr[\"micro\"], lw=lw, label='micro-average ROC curve (AUC = {0:0.2f})'\n                                                   ''.format(roc_auc[\"micro\"]))\ncolors = ['cornflowerblue', 'darkorange', 'forestgreen', 'red', 'purple', 'gray', 'black', 'pink']\nfor i, color in zip(range(8), colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n             label='ROC curve of {0} (AUC = {1:0.2f})'.format(EMOTIONS[i], roc_auc[i]))\n    \nplt.plot([0, 1], [0, 1], color='gray', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate', fontsize=14)\nplt.ylabel('True Positive Rate', fontsize=14)\nplt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=16)\nplt.legend(loc=\"lower right\", fontsize=12)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-20T14:07:00.151311Z","iopub.execute_input":"2023-02-20T14:07:00.152175Z","iopub.status.idle":"2023-02-20T14:07:11.683296Z","shell.execute_reply.started":"2023-02-20T14:07:00.152142Z","shell.execute_reply":"2023-02-20T14:07:11.682092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute classification report\n\nfrom sklearn.metrics import classification_report\n\n# Convert one-hot encoded y_test back to integers\ny_test_int = np.argmax(y_test, axis=1)\n\n# Make predictions\ny_pred = model_4.predict(X_test)\n\n# Convert one-hot encoded y_pred back to integers\ny_pred_int = np.argmax(y_pred, axis=1)\n\n# Generate classification report\nprint(classification_report(y_test_int, y_pred_int))\n","metadata":{"execution":{"iopub.status.busy":"2023-02-20T14:07:11.684895Z","iopub.execute_input":"2023-02-20T14:07:11.685189Z","iopub.status.idle":"2023-02-20T14:07:13.671824Z","shell.execute_reply.started":"2023-02-20T14:07:11.685157Z","shell.execute_reply":"2023-02-20T14:07:13.670589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save the model\n\nmodel_4.save('/kaggle/working/model_4.h5')","metadata":{"execution":{"iopub.status.busy":"2023-02-20T14:07:13.673036Z","iopub.execute_input":"2023-02-20T14:07:13.673296Z","iopub.status.idle":"2023-02-20T14:07:14.162148Z","shell.execute_reply.started":"2023-02-20T14:07:13.673273Z","shell.execute_reply":"2023-02-20T14:07:14.161360Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Detect + predict","metadata":{}},{"cell_type":"code","source":"# function that use MTCNN face detection and pass detected area to our model_4 for recognition\n\nimport cv2\nimport numpy as np\nfrom mtcnn import MTCNN\nfrom IPython.display import Image\n\ndef detect_faces_emo(image_path, detection_confidence=0.99, min_face_size=10):\n    # Load the image using OpenCV\n    image = cv2.imread(image_path)\n\n    # Create an MTCNN detector instance\n    detector = MTCNN()\n\n    # Use the detector to detect faces in the image\n    faces = detector.detect_faces(image)\n\n    # Loop over the detected faces\n    for face in faces:\n        # Check the confidence score of the detection\n        if face['confidence'] < detection_confidence:\n            continue\n        # Extract the bounding box coordinates\n        x, y, width, height = face['box']\n        # Check the size of the bounding box\n        if min(width, height) < min_face_size:\n            continue\n        \n        # Extract the face region from the image\n        face_image = image[y:y+height, x:x+width]\n        # Resize the face image to 96x96\n        face_image_resized = cv2.resize(face_image, (96, 96))\n        #face_image_gray = cv2.cvtColor(face_image_resized, cv2.COLOR_BGR2GRAY)\n        # Reshape the face image to match the input shape of the model\n        face_image_reshaped = face_image_resized.reshape((1, 96, 96, 3))\n        #if np.max(face_image_reshaped) > 1: face_image_reshaped = face_image_reshaped / 255\n        \n        # Use the model to predict the emotion of the face\n        predicted_emo = model_4.predict(face_image_reshaped)[0]\n        \n        # Draw the predicted emotion label on the rectangle around the face\n        label = EMOTIONS[np.argmax(predicted_emo)]\n        cv2.putText(image, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n        # Draw a square rectangle around the face\n        face_size = min(width, height)\n        x_center = x + int(width / 2)\n        y_center = y + int(height / 2)\n        x1 = x_center - int(face_size / 2)\n        y1 = y_center - int(face_size / 2)\n        x2 = x_center + int(face_size / 2)\n        y2 = y_center + int(face_size / 2)\n        cv2.rectangle(image, (x1, y1), (x2, y2), (127, 255, 0), 1)\n\n    # Save the image with the detected faces and predicted emotions to a file\n    cv2.imwrite(\"detected_faces.jpg\", image)\n\n    # Return the path to the saved file\n    return \"detected_faces.jpg\"\n\n","metadata":{"execution":{"iopub.status.busy":"2023-02-20T14:07:14.163089Z","iopub.execute_input":"2023-02-20T14:07:14.163275Z","iopub.status.idle":"2023-02-20T14:07:14.174323Z","shell.execute_reply.started":"2023-02-20T14:07:14.163254Z","shell.execute_reply":"2023-02-20T14:07:14.173311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Detect faces in the new image of various emotions\nimage_path = detect_faces_emo('/kaggle/input/testimages/35_emotions.jpeg')\n\n# Display the saved image\nImage(filename=image_path)","metadata":{"execution":{"iopub.status.busy":"2023-02-20T14:07:14.175344Z","iopub.execute_input":"2023-02-20T14:07:14.176403Z","iopub.status.idle":"2023-02-20T14:07:28.826866Z","shell.execute_reply.started":"2023-02-20T14:07:14.176376Z","shell.execute_reply":"2023-02-20T14:07:28.825950Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test on RAW images and have fun :)","metadata":{}},{"cell_type":"code","source":"# same function but adjusted rectangle text size for big images from my iPhone\n\nimport cv2\nimport numpy as np\nfrom mtcnn import MTCNN\nfrom IPython.display import Image\n\ndef detect_faces_emo(image_path, detection_confidence=0.99, min_face_size=10):\n    # Load the image using OpenCV\n    image = cv2.imread(image_path)\n\n    # Create an MTCNN detector instance\n    detector = MTCNN()\n\n    # Use the detector to detect faces in the image\n    faces = detector.detect_faces(image)\n\n    # Loop over the detected faces\n    for face in faces:\n        # Check the confidence score of the detection\n        if face['confidence'] < detection_confidence:\n            continue\n        # Extract the bounding box coordinates\n        x, y, width, height = face['box']\n        # Check the size of the bounding box\n        if min(width, height) < min_face_size:\n            continue\n        \n        # Extract the face region from the image\n        face_image = image[y:y+height, x:x+width]\n        # Resize the face image to 96x96\n        face_image_resized = cv2.resize(face_image, (96, 96))\n        #face_image_gray = cv2.cvtColor(face_image_resized, cv2.COLOR_BGR2GRAY)\n        # Reshape the face image to match the input shape of the model\n        face_image_reshaped = face_image_resized.reshape((1, 96, 96, 3))\n        #if np.max(face_image_reshaped) > 1: face_image_reshaped = face_image_reshaped / 255\n        \n        # Use the model to predict the emotion of the face\n        predicted_emo = model_4.predict(face_image_reshaped)[0]\n        \n        # Draw the predicted emotion label on the rectangle around the face\n        label = EMOTIONS[np.argmax(predicted_emo)]\n        cv2.putText(image, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 1.8, (0, 255, 0), 3)\n        # Draw a square rectangle around the face\n        face_size = min(width, height)\n        x_center = x + int(width / 2)\n        y_center = y + int(height / 2)\n        x1 = x_center - int(face_size / 2)\n        y1 = y_center - int(face_size / 2)\n        x2 = x_center + int(face_size / 2)\n        y2 = y_center + int(face_size / 2)\n        cv2.rectangle(image, (x1, y1), (x2, y2), (127, 255, 0), 2)\n\n    # Save the image with the detected faces and predicted emotions to a file\n    cv2.imwrite(\"detected_faces.jpg\", image)\n\n    # Return the path to the saved file\n    return \"detected_faces.jpg\"\n\n","metadata":{"execution":{"iopub.status.busy":"2023-02-20T14:07:28.827788Z","iopub.execute_input":"2023-02-20T14:07:28.827957Z","iopub.status.idle":"2023-02-20T14:07:28.840067Z","shell.execute_reply.started":"2023-02-20T14:07:28.827936Z","shell.execute_reply":"2023-02-20T14:07:28.839308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Detect faces in the new image of my friends and save the result to a file\nimage_path = detect_faces_emo('/kaggle/input/myfriends/faces1.jpeg')\n\n# Display the saved image\nImage(filename=image_path)","metadata":{"execution":{"iopub.status.busy":"2023-02-20T14:07:28.840954Z","iopub.execute_input":"2023-02-20T14:07:28.841192Z","iopub.status.idle":"2023-02-20T14:07:34.617731Z","shell.execute_reply.started":"2023-02-20T14:07:28.841172Z","shell.execute_reply":"2023-02-20T14:07:34.616990Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Detect faces in the new image of my friends and save the result to a file\nimage_path = detect_faces_emo('/kaggle/input/myfriends/faces2.jpeg')\n\n# Display the saved image\nImage(filename=image_path)","metadata":{"execution":{"iopub.status.busy":"2023-02-20T14:07:34.618683Z","iopub.execute_input":"2023-02-20T14:07:34.618903Z","iopub.status.idle":"2023-02-20T14:07:37.840513Z","shell.execute_reply.started":"2023-02-20T14:07:34.618882Z","shell.execute_reply":"2023-02-20T14:07:37.839601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Detect faces in the new image of my friends and save the result to a file\nimage_path = detect_faces_emo('/kaggle/input/myfriends/faces3.jpeg')\n\n# Display the saved image\nImage(filename=image_path)","metadata":{"execution":{"iopub.status.busy":"2023-02-20T14:07:37.841521Z","iopub.execute_input":"2023-02-20T14:07:37.841708Z","iopub.status.idle":"2023-02-20T14:07:42.932461Z","shell.execute_reply.started":"2023-02-20T14:07:37.841686Z","shell.execute_reply":"2023-02-20T14:07:42.931600Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now let's modify our function to draw the emotion probability out of all 8 emotions\n\nimport cv2\n\ndef detect_faces_emo_2(image_path, detection_confidence=0.99, min_face_size=10):\n    # Load the image using OpenCV\n    image = cv2.imread(image_path)\n\n    # Create an MTCNN detector instance\n    detector = MTCNN()\n\n    # Use the detector to detect faces in the image\n    faces = detector.detect_faces(image)\n\n    # Loop over the detected faces\n    for face in faces:\n        # Check the confidence score of the detection\n        if face['confidence'] < detection_confidence:\n            continue\n        # Extract the bounding box coordinates\n        x, y, width, height = face['box']\n        # Check the size of the bounding box\n        if min(width, height) < min_face_size:\n            continue\n        \n        # Extract the face region from the image\n        face_image = image[y:y+height, x:x+width]\n        # Resize the face image to 96x96\n        face_image_resized = cv2.resize(face_image, (96, 96))\n        # Reshape the face image to match the input shape of the model\n        face_image_reshaped = face_image_resized.reshape((1, 96, 96, 3))\n        # Use the model to predict the emotion of the face\n        predicted_emo = model_4.predict(face_image_reshaped)[0]\n        predicted_emo_sorted = sorted(list(enumerate(predicted_emo)), key=lambda x: x[1], reverse=True)\n        \n        # Extract the predicted probabilities for each emotion category\n        # Extract the predicted probabilities for each emotion category\n        probabilities = [\"{}\".format(round(prob * 100)) for index, prob in predicted_emo_sorted]\n        \n        # Draw the predicted emotion label on the rectangle around the face\n        label = EMOTIONS[np.argmax(predicted_emo)]\n        cv2.putText(image, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 1.8, (0, 255, 0), 3)\n        \n        # Draw a square rectangle around the face\n        face_size = min(width, height)\n        x_center = x + int(width / 2)\n        y_center = y + int(height / 2)\n        x1 = x_center - int(face_size / 2)\n        y1 = y_center - int(face_size / 2)\n        x2 = x_center + int(face_size / 2)\n        y2 = y_center + int(face_size / 2)\n        cv2.rectangle(image, (x1, y1), (x2, y2), (127, 255, 0), 2)\n\n        # Draw a vertical table with the predicted emotion probabilities\n        table_x, table_y = x1, y2 + 20\n        for index, prob in predicted_emo_sorted:\n            table_y += 40\n            emotion = EMOTIONS[index]\n            cv2.putText(image, emotion, (table_x, table_y), cv2.FONT_HERSHEY_SIMPLEX, 1.6, (255, 255, 255), 3)\n            cv2.putText(image, \"{}%\".format(round(prob * 100)), (table_x + 250, table_y), cv2.FONT_HERSHEY_SIMPLEX, 1.4, (255, 255, 255), 3)\n    \n    # Save the image with the detected faces and predicted emotions to a file\n    cv2.imwrite(\"detected_faces.jpg\", image)\n\n    # Return the path to the saved file\n    return \"detected_faces.jpg\"\n","metadata":{"execution":{"iopub.status.busy":"2023-02-20T14:07:42.933687Z","iopub.execute_input":"2023-02-20T14:07:42.933892Z","iopub.status.idle":"2023-02-20T14:07:42.949313Z","shell.execute_reply.started":"2023-02-20T14:07:42.933869Z","shell.execute_reply":"2023-02-20T14:07:42.947929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Detect faces in the new image of my friends and save the result to a file\nimage_path = detect_faces_emo_2('/kaggle/input/myfriends/faces1.jpeg')\n\n# Display the saved image\nImage(filename=image_path)","metadata":{"execution":{"iopub.status.busy":"2023-02-20T14:07:42.951181Z","iopub.execute_input":"2023-02-20T14:07:42.951616Z","iopub.status.idle":"2023-02-20T14:07:48.512833Z","shell.execute_reply.started":"2023-02-20T14:07:42.951578Z","shell.execute_reply":"2023-02-20T14:07:48.512074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Detect faces in the new image of my friends and save the result to a file\nimage_path = detect_faces_emo_2('/kaggle/input/myfriends/faces2.jpeg')\n\n# Display the saved image\nImage(filename=image_path)","metadata":{"execution":{"iopub.status.busy":"2023-02-20T14:07:48.513895Z","iopub.execute_input":"2023-02-20T14:07:48.514169Z","iopub.status.idle":"2023-02-20T14:07:51.662410Z","shell.execute_reply.started":"2023-02-20T14:07:48.514141Z","shell.execute_reply":"2023-02-20T14:07:51.661869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Detect faces in the new image of my friends and save the result to a file\nimage_path = detect_faces_emo_2('/kaggle/input/myfriends/faces3.jpeg')\n\n# Display the saved image\nImage(filename=image_path)","metadata":{"execution":{"iopub.status.busy":"2023-02-20T14:07:51.663306Z","iopub.execute_input":"2023-02-20T14:07:51.664024Z","iopub.status.idle":"2023-02-20T14:07:56.365888Z","shell.execute_reply.started":"2023-02-20T14:07:51.663996Z","shell.execute_reply":"2023-02-20T14:07:56.364903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Detect faces in the new image of my friends and save the result to a file\nimage_path = detect_faces_emo_2('/kaggle/input/myfriends/face8.jpg')\n\n# Display the saved image\nImage(filename=image_path)","metadata":{"execution":{"iopub.status.busy":"2023-02-20T14:08:23.231629Z","iopub.execute_input":"2023-02-20T14:08:23.232001Z","iopub.status.idle":"2023-02-20T14:08:29.563161Z","shell.execute_reply.started":"2023-02-20T14:08:23.231962Z","shell.execute_reply":"2023-02-20T14:08:29.561005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Detect faces in the new image of my friends and save the result to a file\nimage_path = detect_faces_emo_2('/kaggle/input/myfriends/face9.jpg')\n\n# Display the saved image\nImage(filename=image_path)","metadata":{"execution":{"iopub.status.busy":"2023-02-20T14:08:53.952538Z","iopub.execute_input":"2023-02-20T14:08:53.952916Z","iopub.status.idle":"2023-02-20T14:08:59.993605Z","shell.execute_reply.started":"2023-02-20T14:08:53.952892Z","shell.execute_reply":"2023-02-20T14:08:59.992571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Detect faces in the new image of my friends and save the result to a file\nimage_path = detect_faces_emo_2('/kaggle/input/myfriends/face10.jpg')\n\n# Display the saved image\nImage(filename=image_path)","metadata":{"execution":{"iopub.status.busy":"2023-02-20T14:09:38.831902Z","iopub.execute_input":"2023-02-20T14:09:38.832153Z","iopub.status.idle":"2023-02-20T14:09:44.222385Z","shell.execute_reply.started":"2023-02-20T14:09:38.832131Z","shell.execute_reply":"2023-02-20T14:09:44.218671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Well, what to say?!\n\nI am surprized, scared and happy at the same time ))).\n\nIt looks to me that the model is able to detect when we try to fake the emotions ;)\n\nOr mayby I need some sleep, I spent around 5 full days on age and emotions prediction model and testing.\n\nI hope my work will help you in your progress.\n\nPLEASE UPVOTE !","metadata":{}}]}